# -*- coding: utf-8 -*-
"""text_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FwrfgvM9sp4HyC9nm_vUkLse33IIjZbY
"""


# Library imports
import tensorflow as tf

from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

# value initialization
SEED = 0
input_sequence = "On April 14, 2022, business magnate Elon Musk offered to purchase American social media company Twitter Inc., for $43 billion."
MAX_LEN = len(input_sequence)*2

# Tokenizer and loading pretrained model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-large")
GPT2 = TFGPT2LMHeadModel.from_pretrained("gpt2-large", pad_token_id=tokenizer.eos_token_id)
GPT2.summary()

tf.random.set_seed(SEED)
# Greedy search
input_ids = tokenizer.encode(input_sequence, return_tensors='tf')
greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))

# Beam search
beam_output = GPT2.generate(input_ids, max_length=MAX_LEN, num_beams=5, early_stopping=True)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(beam_output[0], skip_special_tokens=True))

# Beam search with no_repeat_ngram_size

beam_output = GPT2.generate(input_ids, max_length=MAX_LEN, num_beams=5, no_repeat_ngram_size=4, early_stopping=True)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(beam_output[0], skip_special_tokens=True))

# Beam search with num_return_sequences
beam_outputs = GPT2.generate(input_ids, max_length=MAX_LEN, num_beams=5, no_repeat_ngram_size=4, num_return_sequences=3, early_stopping=True)


print("Output:\n" + 100 * '-')
for i, beam_output in enumerate(beam_outputs):
  print("{}: {}".format(i, tokenizer.decode(beam_output, skip_special_tokens=True))) # now we have 3 output sequences

tf.random.set_seed(0)
# Sampling method
sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=0)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(sample_output[0], skip_special_tokens=True))

tf.random.set_seed(0)
# Sampling with temperature threshold
sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=0, temperature=0.7)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(sample_output[0], skip_special_tokens=True))

tf.random.set_seed(0)
# Top=k sampling
sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=50)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(sample_output[0], skip_special_tokens=True))

tf.random.set_seed(0)
# Top-p (nucleus) sampling
sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_p=0.92, top_k=0)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(sample_output[0], skip_special_tokens=True))

tf.random.set_seed(0)
# Top-p sampling with num_return_sequences
sample_outputs = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=50, top_p=0.95, num_return_sequences=3)

print("Output:\n" + 100 * '-')
for i, sample_output in enumerate(sample_outputs):
  print("{}: {}".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))